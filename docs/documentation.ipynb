{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "pnV4Beaa5pLh"
   },
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "d879ijtt5pLn"
   },
   "source": [
    "Since Newton invented calculus, differentiating a function has been essential to the advancement of humanity. Calculating the derivative of a function is crucial to finding the extrema for a function and determining zeros for a function, two operations that are central to optimization (1). Optimization is essential, especially due to the exploding popularity of machine learning, where minimizing a loss function is the essential task. Often, we can find the symbolic/analytical solution to the derivative of a function, however this has become increasingly complex and computationally expensive as our functions/equations have grown in size and complexity. Numerically solving differential equations forms a cornerstone of modern science and engineering and is intimately linked with machine learning; however this method suffers from rounding errors and numerical instability. Many of these issues can be solved using Automatic Differentiation (AD) because AD can calculate the exact derivative up to machine precision (2). The logic and processes behind AD enables it to be implemented using computer code, making it easily accessible for use by scientists and mathematicians. This python package will implement the forward mode of AD. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0A_s2Tiq5pLq"
   },
   "source": [
    "# How to use the package"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JyGbNata5pLt"
   },
   "source": [
    "## Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lGrxDmvK5pLw"
   },
   "source": [
    "### Download from PyPI (for basic users)\n",
    "Ideally, the user should have Anaconda installed (https://www.anaconda.com/download/).\n",
    "\n",
    "The user can, but does not need to, create a virtual environment using `virtualenv`. If the user has Anaconda installed, then the user will already have `virtualenv` installed, but if they do not, the user can the user can run the following command in the terminal:\n",
    "\n",
    "    sudo easy_install virtualenv\n",
    "Then, to create a virtual environment, the user runs the following command in the terminal:\n",
    "\n",
    "    virtualenv env\n",
    "To activate the virtual environment, the user runs the following command in the terminal:\n",
    "\n",
    "    source env/bin/activate\n",
    "To deactivate the virtual environment, the user runs the following command in the terminal:\n",
    "\n",
    "    deactivate\n",
    "Whether or not the user uses a virtual environment, they will need to install the package.\n",
    "\n",
    "    pip install autodiff-py\n",
    "All the necessary dependencies will be installed along with `autodiff-py`. The user can then start using the package in their code with an import statement such as\n",
    "\n",
    "    import autodiff as ad\n",
    "    \n",
    "### Download from Github repo (for developers)\n",
    "Ideally, the a developer should have Anaconda installed (https://www.anaconda.com/download/).\n",
    "\n",
    "To install the package, a developer will download or clone the repository onto their local machine. Then, the user can, but does not need to, create a virtual environment using `virtualenv`. If the user has Anaconda installed, then the user will already have `virtualenv` installed. If the user is not using Anaconda, he or she can run the following command in the terminal:\n",
    "\n",
    "    sudo easy_install virtualenv\n",
    "Then, to create a virtual environment, the user runs the following command in the terminal in the package directory:\n",
    "\n",
    "    virtualenv env\n",
    "To activate the virtual environment, the user runs the following command in the terminal in the package directory:\n",
    "\n",
    "    source env/bin/activate\n",
    "To deactivate the virtual environment, the user runs the following command in the terminal in the package directory:\n",
    "\n",
    "    deactivate\n",
    "Whether or not the user uses a virtual environment, he or she will need to have the necessary dependencies. The package directory includes a *requirements.txt* file that lists the necessary dependencies. The user can easily install all the dependencies with the following command:\n",
    "\n",
    "    pip install -r requirements.txt\n",
    "After installing the necessary dependencies, the user can then start using the package in their code with an import statement such as\n",
    "\n",
    "    import autodiff as ad\n",
    "To run the tests for the package, the user can run the following command in terminal in the package folder:\n",
    "\n",
    "    pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#install the package autodiff-py\n",
    "!pip install autodiff-py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0taiOCzQ5pLz"
   },
   "source": [
    "## Usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YWvClCF35pL3"
   },
   "source": [
    "\n",
    "\n",
    "For example, suppose $f(x, y) = x^2 + 2xy + y^2$, and $x = 1, y = 2$.\n",
    "​\n",
    "$\n",
    "f(1, 2) = 9\\\\\n",
    "\\frac{\\partial f}{\\partial x} = 2x + 2y \\implies \\frac{\\partial f}{\\partial x}|_{x = 1, y = 2} = 6 \\\\\n",
    "\\frac{\\partial f}{\\partial y} = 2x + 2y \\implies \\frac{\\partial f}{\\partial y}|_{x = 1, y = 2} = 6 \n",
    "$\n",
    "​\n",
    "\n",
    "\n",
    "To solve this problem using our package, the user will run the following straightforward code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SExfkhvS8LDe"
   },
   "outputs": [],
   "source": [
    "import autodiff as ad\n",
    "\n",
    "# Define x and y scalars with variable name and value. The actual python variable names do not matter.\n",
    "x = ad.Scalar('x', 1)\n",
    "y = ad.Scalar('y', 2)\n",
    "# Express f in terms of x and y. User does not need to define any more scalar objects besides the basic ones x and y above. \n",
    "f = x ** 2 + 2 * x * y + y ** 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "Oqb7x1DTYeF4",
    "outputId": "946b6022-926c-424f-cb22-e487dbc707e8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get value of f(1, 2)\n",
    "f.getValue()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "NFfojdsiYeF8",
    "outputId": "a4a1c638-865e-4986-b882-925e980c2a87"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get partial derivative with respect to x. Pass in variable names as String in list.\n",
    "f.getGradient(['x'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zuXsoYmIYeF_",
    "outputId": "7703a8c2-4ea9-4834-de6f-8a58c364beeb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6.])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get partial derivative with respect to y\n",
    "f.getGradient(['y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "d9h3eKdpYeGD",
    "outputId": "a15c6fcc-08a4-4fcf-9c01-bc69c56281c6"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([6., 6.])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get both partial derivatives\n",
    "f.getGradient(['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "wNEjgcxeYeGH",
    "outputId": "4abf354e-3263-45f0-f84c-a4725e863c00"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'y': 6.0, 'x': 6.0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get all partial derivatives for the function 'f'\n",
    "f.getDeriv()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LPAESup_72Im"
   },
   "source": [
    "Also included in our package is an optimization library, `optimize`. The package includes Newton's method for root finding and also Quasi-Newton's methods and gradient descent for minimum finding. The benefit of this package is that the user does not need to use any functionality related to `autodiff-py` themselves; they just need to define the function (which only takes in one required argument) they want to optimize/find the root of and give the initial guess as a list/array. \n",
    "\n",
    "For example, suppose we want to find the root of the function \n",
    "$\\vec{f}(x_1, x_2, x_3) = [2x_1 + 1, -2x_1 - x_2^2 + 3, -3 x_3 + 1]$. The roots of this function will be \n",
    "$(x_1, x_2, x_3) = (-0.5, 2, 1/3)$ and $(x_1, x_2, x_3) = (-0.5, -2, 1/3)$. We can use the function `newtons_method` to solve for a root. The root found will change depending on the initial starting position. We can also specify the method used to calculate the step size for each iteration in `newtons_method`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "whFFa0Xm8ACf"
   },
   "outputs": [],
   "source": [
    "import autodiff.optimize as optimize\n",
    "import numpy as np\n",
    "\n",
    "def f(x):\n",
    "    x1 = x[0]\n",
    "    x2 = x[1]\n",
    "    x3 = x[2]\n",
    "    return np.array([2 * x1 + 1, -2 * x1 - x2 ** 2 + 3, -3 * x3 + 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "H3L0CkiNlJKa",
    "outputId": "7ef4c689-fdc6-4f42-95ef-ee108acc7399"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5       ,  2.        ,  0.33333333])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns two elements in a tuple. First element corresponds to position of root, the second the number of iterations\n",
    "#it took to converge\n",
    "#First root\n",
    "optimize.newtons_method(f, [1, 1, 0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "BoghNdjIlLsu",
    "outputId": "2f2d2be6-44bc-4ee5-ec71-d6efbfff90a9"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5       , -2.        ,  0.33333333])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Second root\n",
    "optimize.newtons_method(f, [1, -1, 0])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "J52dydemlNkh",
    "outputId": "2990f6b7-5e78-4572-90ce-36a8a35daf8e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5       ,  2.        ,  0.33333333])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Use gmres action step update method\n",
    "optimize.newtons_method(f, [1, 1, 0], method = 'gmres_action')[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "K2sKYOGolP8M",
    "outputId": "cf2b4f85-829e-445b-889a-3b851d7895f8"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.5       , -2.        ,  0.33333333])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize.newtons_method(f, [1, -1, 0], method = 'gmres_action')[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UZDgTKENlXNr"
   },
   "source": [
    "Suppose we want to find the minimum of the function $f(x_1, x_2) = 2(x_1-1)^2+3x_2^2$, which is found at $(x_1, x_2) = (1, 0)$. We can use the function `gradient_descent` or `quasi_newtons_method` depending on the method we want. `quasi_newtons_method` can solve for the minimum using one of the specified Quasi-Newton's methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1ZYVspJ_lRoC"
   },
   "outputs": [],
   "source": [
    "def f(x):\n",
    "    return 2 * ((x[0] - 1) ** 2) + 3 * (x[1] ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "7QgXxWYNlgjD",
    "outputId": "5ef0c3e5-9b4a-41d4-fa86-eb74c92869ab"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.00000000e+000, 7.78609779e-268])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Returns two elements in a tuple. First element corresponds to position of minimum, the second the number of iterations\n",
    "#it took to converge\n",
    "#Gradient descent\n",
    "optimize.gradient_descent(f, [11, 41])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-JT_FfHM5pL7"
   },
   "source": [
    "# Background\n",
    "​\n",
    "The following mathematical concepts and background are required for understanding automatic differentiation:\n",
    "​\n",
    "### 1. Differential calculus\n",
    "​\n",
    "Differential calculus is a subfield of calculus concerned with the study of the rates at which quantities change.\n",
    "For example, given the function: \n",
    "\\begin{align}\n",
    " f\\left(x\\right) &=  {x^{2}}     \n",
    " \\end{align}\n",
    " \n",
    " Increment x by h:\n",
    " \\begin{align}\n",
    " f\\left(x+h\\right) &=  {(x+h)^{2}}     \n",
    " \\end{align}\n",
    " \n",
    " Apply the finite difference approximation to calculate the slope:\n",
    "  \\begin{align}\n",
    " \\frac{f\\left(x+h\\right) - f\\left(x\\right) }{h}\n",
    " \\end{align}\n",
    " \n",
    "Simplify the equation:\n",
    "  \\begin{align}\n",
    " &= \\frac{x^{2}+2xh+h^{2}-x^{2} }{h}\\\\\n",
    " &= \\frac{2xh+h^{2}}{h}\\\\\n",
    " &=2x+h\n",
    " \\end{align}\n",
    " \n",
    " Set $h\\rightarrow 0$:\n",
    "   \\begin{align}\n",
    " 2x +0 &= 2x\n",
    "  \\end{align}\n",
    "  \n",
    "The derivative is then defined is:\n",
    "\\begin{align}\n",
    " \\lim_{h\\to0} \\frac{f\\left(x+h\\right) - f\\left(x\\right) }{h}\n",
    " \\end{align}\n",
    " \n",
    "### 2. Elementary functions and their derivatives\n",
    "\n",
    "|       Function $f(x)$                |       Derivative $f^{\\prime}(x)$                |\n",
    "| :-------------------:  | :------------------------------------------------------------------------------:  |\n",
    "| ${c}$           | $0$         |\n",
    "| ${x}$           | $1$         |\n",
    "| ${x^{n}}$           | ${nx^{n-1}}$         |\n",
    "| $\\frac{1}{x}$     | $\\frac{-1}{x^{2}}$     |\n",
    "| $ln{x}$     | $\\frac{1}{x}$     |\n",
    "| $\\sin(x)$           |   $\\cos(x)$         |\n",
    "| $\\cos(x)$           |   $-\\sin(x)$         |\n",
    "| $\\tan(x)$           |   $\\dfrac{1}{\\cos^2(x)}$         |\n",
    "| $\\exp(x)$           |   $\\exp(x)$         |\n",
    "| ${a^{x}}$           |   ${a^{x}\\ln{a}}$         |\n",
    " \n",
    "### 3. The chain rule$^{(1)}$\n",
    "\n",
    "For a function $h(u(t))$, the derivative of $h$ with respect to $t$ can be expressed as:\n",
    "$$\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$$\n",
    "If the function is expressed as a combination of multiple variables that are expressed in terms of t, i.e. $h(u(t), v(t))$, the the derivative of $h$ with respect to $t$ can be expressed as:\n",
    "$$\\frac{\\partial h}{\\partial t} = \\frac{\\partial h}{\\partial u}\\frac{\\partial u}{\\partial t} + \\frac{\\partial h}{\\partial v}\\frac{\\partial v}{\\partial t}$$\n",
    "\n",
    "Note that we are only looking at scalar variables in this case, but this idea can be extended to vector variables as well.\n",
    "\n",
    "  For any $h\\left(y\\left(x\\right)\\right)$ where $y\\in\\mathbb{R}^{n}$ and $x\\in\\mathbb{R}^{m}$,\n",
    "  \n",
    "  \\begin{align}\n",
    "    \\nabla_{x}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}.\n",
    "  \\end{align}\n",
    "\n",
    "### 4. The graph structure of calculations and forward accumulation\n",
    "\n",
    "Forward accumulation is computing the derivative using the chain rule starting from the inner most derivative to the outer most derivative, where we assume the most basic variables have seed values. Using a graph helps visualize forward accumulation. For example,\n",
    "\n",
    "\\begin{align}\n",
    " f\\left(x,y\\right) &= \\frac{x}{y} +cos(x)sin(y)\\\\\n",
    " x &= y = 1\n",
    "\\end{align}\n",
    "\n",
    " \n",
    "![](img/graph_eg.png)\n",
    "\n",
    "| Trace | Elementary Function | Current Value | Elementary Function Derivative | &nbsp; &nbsp; $\\nabla_{x}$ Value &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;| &nbsp; &nbsp; $\\nabla_{y}$ Value &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;\n",
    "| :---: | :-----------------: | :-----------: | :----------------------------: | :-----------------: | :-----------------: | :-----------------: |\n",
    "| $w_{1}$ | $1$ | $1$ | $\\dot{w_1}$ | $1$ | $0$ |\n",
    "| $w_{2}$ | $1$ | $1$ | $\\dot{w_2}$ | $0$ | $1$ |\n",
    "| $w_{3}$ | $cos{(w_1})$ | $cos{(1)}$ | $-sin{(w_1)}\\dot{w_1}$ | $-sin(1)$ | $0$ |\n",
    "| $w_{4}$ | $sin{(w_2})$ | $sin{(1)}$ | $cos{(w_2)}\\dot{w_2}$ | $0$ | $cos{(1)}$ |\n",
    "| $w_{5}$ | $w_3\\dot w_4$ | $sin{(1)}cos{(1)}$ | $w_4\\dot{w_3} + w_3\\dot{w_4}$ | $-sin^2{(1)}$ | $cos^2{(1)}$ |\n",
    "| $w_{6}$ | $w_1 / w_2$ | $1$ | $\\dot{w_1}/w_2 - w_1 \\dot{w_2}/ w_2^2$ | $1$ | $-1$ |\n",
    "| $w_{7}$ | $w_5 + w_6$ | $sin{(1)}cos{(1)} + 1$ | $\\dot{w_5} + \\dot{w_6}$ | $-sin^2{(1)} + 1$ | $cos^2{(1)}-1$ |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-6NTaMMEyQ3z"
   },
   "source": [
    "### 5. Optimization\n",
    "\n",
    "Often times, we want to optimize a convex function, which is a function where a line segment between any two points on the graph of the function lies above or on the graph. A differentiable function is at a critical point when the derivative/gradient is equal to $0$. For convex functions, there is a minimum at the critical point. Thus, we are interested in finding when the derivative/gradient of a function is equal to $0$. \n",
    "\n",
    "It is important to understand that many optimization/root-finding methods are iterative in nature; in iteration $k$ the position is changed using step: \n",
    "\n",
    "$x_{k+1} = x_{k} + \\Delta x_k$. \n",
    "\n",
    "In root-finding methods, we can solve for $\\Delta x_k$ using the following equation:\n",
    "\n",
    "$J_f\\Delta x_k  = -f(x_k)$\n",
    "\n",
    "where $J_f$ is the Jacobian, the matrix of first order derivatives, of $f(x)$ at iteration $k$.\n",
    "\n",
    "Similarly, in most optimization methods, we can solve for $\\Delta x$ using the following equation:\n",
    "\n",
    "$B_k\\Delta x_k = -\\alpha \\nabla f(x_k)$\n",
    "\n",
    "where $B$ is the Hessian, the matrix of second order derivatives, of $f(x)$ at iteration $k$, $\\alpha$ is found through backtracking line search, and $\\nabla f(x)$ is the gradient of $f(x)$\n",
    "\n",
    "In small cases, solving for the inverse of $J$ and $H = B^{-1}$ would not be a problem. However, as the dimension gets larger and larger, solving for the inverse becomes very computationally intensive. As such, one must find other ways of solving for $\\Delta x$. One could use methods such as LU decomposition or the Generalized minimal residual method (Gmres) to solve for it. \n",
    "\n",
    "For optimization, one could also use Quasi-Newton methods to estimate the inverse of $B$. At each iteration, the inverse of the Hessian can be approximated using an update equation. For example, for DFP, to estimate $H_k$, the inverse of $B_k$, \n",
    "\n",
    "$\n",
    "H_k = H_k + \\frac{\\Delta x_k \\Delta x_k^T}{\\Delta x_k^T y_k} - \\frac{H_k y_k y_k^T H_k}{y_k^T H_k y_k}\n",
    "$\n",
    "\n",
    "where $y_k = \\nabla f(x_{k+1}) -  \\nabla f(x_{k})$\n",
    "\n",
    "More update equations can be found on Wikipedia's Quasi-Newton method page (3)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7df0AYzM5pL-"
   },
   "source": [
    "# Software Organization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CuVg_pPY5pMB"
   },
   "source": [
    "The directory structure is:      \n",
    " \n",
    " ```\n",
    " autodiff\\\n",
    "          autodiff\\\n",
    "                    __init__.py\n",
    "                    functions.py\n",
    "                    scalar.py\n",
    "                    optimize.py\n",
    "                    vector.py\n",
    "          tests\\\n",
    "                    test_composite.py\n",
    "                    test_functions.py\n",
    "                    test_scalar.py \n",
    "                    test_optimize.py\n",
    "                    test_vector.py\n",
    "          docs\\\n",
    "                    milestone1.ipynb\n",
    "                    milestone2.ipynb\n",
    "                    Final_milestone.ipynb\n",
    "          README.md\n",
    "          requirements.txt\n",
    "          setup.cfg\n",
    "          LICENSE\n",
    "               \n",
    " ```   \n",
    "\n",
    "**Basic modules and what they do?**\n",
    "\n",
    "This module aims to compute forward automatic differentiation. The module we implemented can efficiently compute the derivatives of a function with automatic differentiation of a scalar input. The *scalar.py* file contains the objects which compute the value and the derivative of scalar variables. The dunder methods add, sub, mul, truediv, pow, iadd, isub, imul, idiv, ipow are implemented in this module. The *function.py* file contains sine, cosine, power, exponential, and other mathematical functions. Both *scalar.py* and *function.py* return the value and the derivative.\n",
    "\n",
    "The *optimize.py* file contains functions for find the roots and minima of user-defined functions.\n",
    "\n",
    "<br/>\n",
    "**Where do tests live? How are they run? How are they integrated?**\n",
    "\n",
    "We are using both `TravisCI` and `Coveralls` to test our module. Each test function exists in the tests folder, and it utilizes the `pytest` package.\n",
    "\n",
    "<br/>\n",
    "**How can someone install your package?**\n",
    "\n",
    "Our package has been released in `PyPI` under the name `autodiff-py`. Therefore, our auto-differentiating software can be downloaded at https://github.com/cs207FinalProjectGroup/cs207-FinalProject.git."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "t1BepQX05pME"
   },
   "source": [
    "# Implementation Details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o2x4oG0p5pMJ"
   },
   "source": [
    "## *Scalar* Class\n",
    "\n",
    "This package relies on the *Scalar* class, which represents scalar variables. To initialize a *Scalar* class object, the user will pass in a string that represents the variable name (i.e. 'x', 'y', 'x1', etc.) and the value of the variable. The user also has the option to specify the starting value for the derivative; if a starting value for the derivative is not specified, then it defaults to $1$. A *Scalar* object holds two attributes: 1) the value of the variable `_val` at the current step and 2) a dictionary `_deriv` containing the derivative or partial derivatives (keys will be the names of the variables (i.e *x* and *y*) and the values will be the derivative value with respect to each variable). The user should utilize the methods `getValue()` and `getDeriv()` to obtain the value and derivative(s) of their Scalar object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "bJY3EJbh5pMM",
    "outputId": "54ac60f0-a0f1-42ac-e23d-ba720193602d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value of x:  2.0\n",
      "Derivative of x: 1.0\n",
      "Value of y:  5.0\n",
      "Derivative of y:  3.2\n"
     ]
    }
   ],
   "source": [
    "import autodiff as ad\n",
    "\n",
    "x = ad.Scalar('x',2);\n",
    "print('Value of x: ', x.getValue()); #should be 2.0\n",
    "print('Derivative of x:', x.getDeriv()['x']); #should be 1.0\n",
    "\n",
    "y = ad.Scalar('y', val=5, deriv=3.2);\n",
    "print('Value of y: ', y.getValue()); #should be 5.0\n",
    "print('Derivative of y: ', y.getDeriv()['y']); #should be 3.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CyaOFePD5pMh"
   },
   "source": [
    "Storing the partial derivatives with respect to each variable allows us to easily compute additional derivatives with respect to each variable when we are performing mathematical operations because we can update each partial derivative individually. When a *Scalar* object is initialized, by default `_deriv` will just be a dictionary with the only key being the string the user passes in with value 1. A user can access the value of a *Scalar* object using the *getValue()* method and access the derivative (or partial derivatives) for the object through the *getDeriv()* method. The user can also get the derivatives/partial derivatives as a numpy array with the *getGradient()* method, which takes in a list of strings, with each element representing the variable to take the derivative with respect to, as an argument. \n",
    "\n",
    "The dunder methods __add__, __sub__, __mul__,  __truediv__, __pow__, __iadd__, __isub__, __imul__, __itruediv__, __ipow__ (and the right equivalents for the ones that have one) have been overwritten so that they return a new *Scalar* object with an updated value and derivatives. Thus, the adding or substracting two *Scalars* or raising a *Scalar* to the power of another *Scalar* does not change the values or derivatives of the original *Scalar* objects. By overwriting these methods, we are implementing forward accumulation, as the orders of operation allows us to traverse the chainrule starting from the inside."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "8jPmYsEa5pMk",
    "outputId": "026470b4-ef28-4306-fa9b-22edbb8ffe79"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.0\n",
      "1.0\n",
      "1.0\n",
      "\n",
      "2.0 , 5.0\n"
     ]
    }
   ],
   "source": [
    "#addition example\n",
    "x,y = ad.Scalar('x', 2),ad.Scalar('y', 5);\n",
    "val = x+y;\n",
    "print(val.getValue()); #should be 7.0\n",
    "print(val.getDeriv()['x']); #should be 1.0\n",
    "print(val.getDeriv()['y']); #should be 1.0\n",
    "print();\n",
    "print(x.getValue(),',', y.getValue()); #x, y should retain original values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "ToV1H5HX5pMx",
    "outputId": "b876e2fa-f429-47af-fe91-298aae4ea017"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.0\n",
      "4.0\n"
     ]
    }
   ],
   "source": [
    "#pow example\n",
    "x=ad.Scalar('x', 2);\n",
    "val = x**2;\n",
    "print(val.getValue()) #should be 4.0\n",
    "print(val.getDeriv()['x']) #should be 4.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "55RW4-oU5pM6",
    "outputId": "5bb538c9-4692-4857-fcbb-da6158f0daa3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.5\n",
      "0.5\n",
      "-0.75\n",
      "\n",
      "3.0 , 2.0\n"
     ]
    }
   ],
   "source": [
    "#division example\n",
    "x,y=ad.Scalar('x', 3),ad.Scalar('y', 2);\n",
    "val = x/y;\n",
    "print(val.getValue()) # should be 1.5\n",
    "print(val.getDeriv()['x']); #should be 0.5\n",
    "print(val.getDeriv()['y']); #should be -0.75\n",
    "print();\n",
    "print(x.getValue(),',', y.getValue()); #x, y should retain original values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "GuJ0x_r-5pNC"
   },
   "source": [
    "We have also implemented the functions **sin**, **cos**, **tan**, **arcsin**, **arccos**, **arctan**, **log**, **ln**, **power**, and **exp**. All of these functions can take in an `int`, `float`, or *Scalar* object. If only an `int`/`float` is provided, then a `float` is returned, but if *Scalar* object is provided, then a new *Scalar* object. No changes are made to the value or derivatives of the original *Scalar* object passed in. Within these functions, the *numpy* functions *sin*, *cos*, *tan*, *arcsin*, *arccos*, *arctan*, *log*, *ln* and *exp* are used to calculate the appropriate values. <br/>\n",
    "\n",
    "The trigonometric functions  **sin(x)**, **cos(x)**, and **tan(x)** each take in an `int`, `float`, or *Scalar* object *x* and apply the respective trigonometric function to the *x*. If a *x* is a *Scalar* object, the trigonometric function is applied to the *x._val* attribute and the derivative(s) is updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "CxezEuU75pNK",
    "outputId": "485411cf-b16e-4014-8f82-cbadf764c9ca"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9092974268256817\n",
      "-0.4161468365471424\n",
      "1.0\n",
      "-0.0\n",
      "-2.185039863261519\n",
      "5.774399204041917\n"
     ]
    }
   ],
   "source": [
    "import autodiff as ad\n",
    "import numpy as np \n",
    "\n",
    "# Sine functions\n",
    "x = ad.Scalar('x', 2.0) # float input \n",
    "val = ad.sin(x)\n",
    "print(val.getValue()); #shoud be 0.90929742682\n",
    "print(val.getDeriv()['x']); #should be -0.41614683654\n",
    "\n",
    "\n",
    "# Cosine functions\n",
    "x = ad.Scalar('x', 0) # integer input \n",
    "val = ad.cos(x)\n",
    "print(val.getValue()); #should be 1.0\n",
    "print(val.getDeriv()['x']); #should be 0.0\n",
    "\n",
    "# tan function\n",
    "x = ad.Scalar('x', 2.0); # float input \n",
    "val = ad.tan(x);\n",
    "print(val.getValue()); #shoudl be -2.185039863261519\n",
    "print(val.getDeriv()['x']); #should be 5.7743992040419174"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NNaQ_4oY5pNP"
   },
   "source": [
    "The **power(x1, x2)** function raises *x1* to the power of *x2*. *x1* and *x2* can be any combination of `ints`, `floats`, or *Scalar* objects. If only ints and floats are provided, then **power** will return a `float` with value *x1* raised to the power of *x2*. If at least one *Scalar* object is provided, then **power** works just like using the **__pow__** operator and returns a new *Scalar* object without changing any values in the original *Scalar* object(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "JfTnchWP5pNR",
    "outputId": "8b5d7ee5-9f65-4014-d841-2176c77a5497"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "125.0\n"
     ]
    }
   ],
   "source": [
    "#power example\n",
    "x = 5.0\n",
    "p = 3.0\n",
    "print (ad.power(x, p)) # should be 125.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NN4QrUvIlJQ-"
   },
   "source": [
    "The **sqrt(x)** function raises *x* to the power of 0.5. *x1* can be a an `int`, `float`, or *Scalar* objects. If only an `int` or `float` is provided, then **sqrt** will return a `float` with value *x* raised to the power of 0.5. If a *Scalar* object is provided, then **sqrt** works just like using the **__pow__** operator and returns a new *Scalar* object without changing any values in the original *Scalar* object(s).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "74pj484D5u08",
    "outputId": "8dd9134d-38f7-4c8a-85f4-e7ef9f94f83d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.0\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# sqrt example\n",
    "\n",
    "b = ad.Scalar('b', 4)\n",
    "b_sqrt = ad.sqrt(b)\n",
    "b = ad.Scalar('b', 4)\n",
    "b_sqrt = ad.sqrt(b)\n",
    "print (b_sqrt.getValue()); #should be 2.0\n",
    "print (b_sqrt.getDeriv()['b']) #should be 0.25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NEIwIs4J5pNW"
   },
   "source": [
    "The **exp(x)** function raises *e* to the power of *x* , where *x* can be an `int`, `float`, or *Scalar* object. If *x* is an `int` or `float`, then a `float` with value equal to *e* raised to the power of *x* is returned. If *x* is a Scalar object, then a new *Scalar* is returned with an updated value and derivative(s). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "52nEL7ix5pNX",
    "outputId": "b1587a8e-1183-43b4-8b5a-3163a66c7634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.718281828459045\n"
     ]
    }
   ],
   "source": [
    "#exponential example\n",
    "x = 1.0\n",
    "print (ad.exp(x)) # should be 2.718281828459045"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KapgG5Dm2knY"
   },
   "source": [
    "The **log(x, base)** function give the log of *x*, where *x* can be an int, float, or Scalar object. This function takes in an `int`, `float`, or *Scalar* object for *x* and an `int` or `float` for *base*. It applies log with base *base* to the to the value of *x* and updates the derivative appropriately. If the argument is an `int` or `float`, then the function returns a `float`. If the argument is a *Scalar* object, the function returns a new *Scalar* object with the updated value and derivative. The **ln(x)** function returns the log with base $e$ of *x* and behaves identically to  **log(x, base)**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JEbGZK1J29ks",
    "outputId": "01a400c3-cabf-421d-f898-a3bde6e57ce0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "0.043429448190325175\n",
      "1.0\n",
      "0.36787944117144233\n"
     ]
    }
   ],
   "source": [
    "# log functions \n",
    "\n",
    "x = ad.log(ad.Scalar('x', 10), 10)\n",
    "print (x.getValue()) # shoud be 1.0\n",
    "print (x.getDeriv()['x']) # should be 0.043429448190325175\n",
    "\n",
    "x = ad.ln(ad.Scalar('x', np.e))\n",
    "print (x.getValue()) # should be 1.0\n",
    "print (x.getDeriv()['x']) # should be 0.36787944117144233"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-KMo6Sq550FC"
   },
   "source": [
    "The **logistic(x)** function takes in an `int`, `float`, or *Scalar* object and applies the logistic function to the value. If the argument is an `int` or `float`, then the function returns a `float`. If the argument is a *Scalar* object, the function returns a new *Scalar* object with the updated value and derivative.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "JNZgJ1zF50zY",
    "outputId": "e0f7f502-2ebf-49ca-e40e-b67cb98bce1e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9996646498695336\n",
      "0.0003352376707564743\n",
      "0.5\n",
      "0.25\n"
     ]
    }
   ],
   "source": [
    "# logistic example\n",
    "\n",
    "x = ad.Scalar('x', 8)\n",
    "y = ad.logistic(x)\n",
    "print (y.getValue())\n",
    "print(y.getDeriv()['x'])\n",
    "\n",
    "x = ad.Scalar('x', 0)\n",
    "y = ad.logistic(x)\n",
    "print(y.getValue())\n",
    "print(y.getDeriv()['x'])   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X5hd98qEn-Qr"
   },
   "source": [
    "We have also implemented the functions **sinh**, **cosh**, and **tanh**. All functions work with `np.sinh`, \n",
    "`np.cosh`, `np.tanh` to calculate the appropriate values. \n",
    "\n",
    "The hyperbolic functions **sinh(x)**, **cosh(x)**, and **tanh(x)** each take in an in `int`, `float`, or *Scalar* object *x* and apply the resprective hyperbolic functions to the *x*. If a *x* is a *Scalar* object, the hyperbolic function is applied to the *x._val* attribute and the derivative(s) is updated accordingly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "G-2zjwMPoBih",
    "outputId": "d028ad24-2350-4b50-a475-bba341c7119c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.6268604078470186\n",
      "3.7621956910836314\n",
      "1.0\n",
      "0.0\n",
      "0.9640275800758169\n",
      "0.07065082485316443\n"
     ]
    }
   ],
   "source": [
    "# Sinh functions\n",
    "x = ad.Scalar('x', 2.0) # float input \n",
    "val = ad.sinh(x)\n",
    "print(val.getValue());  # shoud be 3.6268604078470186\n",
    "print(val.getDeriv()['x']); # should be 3.7621956910836314\n",
    "\n",
    "# Cosh functions\n",
    "x = ad.Scalar('x', 0) # integer input \n",
    "val = ad.cosh(x)\n",
    "print(val.getValue()); # should be 1.0\n",
    "print(val.getDeriv()['x']); # should be 0.0\n",
    "\n",
    "# tanh function\n",
    "x = ad.Scalar('x', 2.0); # float input \n",
    "val = ad.tanh(x);\n",
    "print(val.getValue()); # shoudl be 0.9640275800758169\n",
    "print(val.getDeriv()['x']); # should be 0.07065082485316443"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SEFOE5hCoFCq"
   },
   "source": [
    "The __eq__ and __ne__ dunder methods have been implemented to compare *Scalar* objects.\n",
    "__eq__ checks if two *Scalar* objects have equivalent values for both the value and the derivative. __ne__ checks if two *Scalar* objects are not equal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "tjPf-rxnoLU0",
    "outputId": "a26c754f-c25f-452e-dfad-2284bef884fe"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "True\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# __eq__, __ne__ example\n",
    "\n",
    "x = ad.Scalar('x', 1)\n",
    "x2 = ad.Scalar('x', 1)\n",
    "print (x == x2) # should be true \n",
    "print (not (x != x2)) # should be true \n",
    "\n",
    "x = ad.Scalar('x', 1) \n",
    "x2 = ad.Scalar('x', 2.0) \n",
    "print (x != x2) # should be true \n",
    "print (not (x == x2)) # should be true "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OBYobxqZZVsh"
   },
   "source": [
    "## *Vectors*\n",
    "\n",
    "There is not an individual class for vectors/arrays. Users can create a vector using the `create_vector` function, which takes in a name for the vector and a list/array of starting values for each *Scalar* object in the vector. This function creates a `numpy` array of *Scalar* objects that is the same length as the list/array of starting values that is passed in. Each *Scalar* object in the vector is given a name equivalent to the name of the vector concatenated with the index of the object in the vector plus one (e.g. if a list `[1,2,3]` is passed in for the starting values and the vector is name `'v'`, then the *Scalar* objects are name `'v1'`, `'v2'`, and `'v3'`).  A user can also specify the starting derivative value for each *Scalar* object in the vector/array through the `seed_vector` argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "0iYY7LQ2ihy0",
    "outputId": "10552e6a-906a-4022-e0f0-0105147c1495"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "3.1\n",
      "2.0\n",
      "4.2\n"
     ]
    }
   ],
   "source": [
    "v = ad.create_vector('v', values=[1, 2], seed_vector=[3.1, 4.2]);\n",
    "print(v[0].getValue()) # should print 1.0\n",
    "print(v[0].getDeriv()['v1']); #should print 3.1\n",
    "\n",
    "print(v[1].getValue()) # should print 2.0\n",
    "print(v[1].getDeriv()['v2']); #should print 4.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UayatjvaxCBT"
   },
   "source": [
    "A user can use the function `get_value(vector)` to get the values of all of the *Scalar* objects in the vector. The function returns a `numpy` array of the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "VaBlrjaAxGHx",
    "outputId": "b92a6951-457d-47e6-a402-b02642bf2613"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 2.])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.get_value(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zsq7CpYzxJyl"
   },
   "source": [
    "To obtain the derivatives (or partial derivatives) for each *Scalar* object in the vector, the user can use the `get_deriv(vector)` function. This will return a `numpy` array of dictionaries containing the derivatives (or partial derivatives) for each *Scalar* object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "phazihiwxMzz",
    "outputId": "36d2c5d9-4329-4db5-f5d5-9a111a30490e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([{'v1': 3.1}, {'v2': 4.2}], dtype=object)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ad.get_deriv(v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HjbJ4DyqxarR"
   },
   "source": [
    "Since a vector is just a `numpy` array of *Scalar* objects, all of the dunder methods implemented for the *Scalar* class (__add__, __sub__, __mul__,  __truediv__, __pow__, __iadd__, __isub__, __imul__, __idiv__, __ipow__, and the right equivalents) will be used when the same operations are done on the vector. Similar to `numpy` methods, the operations are conducted element-wise (i.e. in an addition operation between a vector of *Scalar* objects and a float, the float is added to each *Scalar* object). Each operation generates a new vector of *Scalar* objects with updated values and derivatives, which importantly leaves the vector to which the operation was applied unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "KpJHeuUQxgEX",
    "outputId": "2427c861-1dc3-43f2-c60f-517a4b15498b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for v:  [1. 2.]\n",
      "Derivatives for v:  [{'v1': 3.1} {'v2': 4.2}]\n",
      "Values for w:  [ 4. 12.]\n",
      "Derivatives for w:  [{'v1': 6.2} {'v2': 8.4}]\n"
     ]
    }
   ],
   "source": [
    "#demonstration for addition with vectors of Scalar objects\n",
    "w = (v + [1,4]) * 2;\n",
    "\n",
    "print('Values for v: ', ad.get_value(v));\n",
    "print('Derivatives for v: ', ad.get_deriv(v));\n",
    "\n",
    "print('Values for w: ', ad.get_value(w));\n",
    "print('Derivatives for w: ', ad.get_deriv(w));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "I1xuWE6j2A5F"
   },
   "source": [
    "The mathematical functions **sin**, **arccos**, **sqrt**, **power**, **ln**, and others can also be applied to a vector of *Scalar* objects. Like the dunder methods for *Scalar* objects, these functions will be applied element-wise and generate a new vector of updated *Scalar* objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "qv8jlUAk2IGT",
    "outputId": "2877bdbe-6375-4c27-fe25-6b3b455947ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values for v:  [1. 2.]\n",
      "Derivatives for v:  [{'v1': 3.1} {'v2': 4.2}]\n",
      "Values for z:  [0.84147098 0.90929743]\n",
      "Derivatives for z:  [{'v1': 1.6749371481912334} {'v2': -1.7478167134979983}]\n"
     ]
    }
   ],
   "source": [
    "z = ad.sin(v);\n",
    "\n",
    "print('Values for v: ', ad.get_value(v));\n",
    "print('Derivatives for v: ', ad.get_deriv(v));\n",
    "\n",
    "print('Values for z: ', ad.get_value(z));\n",
    "print('Derivatives for z: ', ad.get_deriv(z));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ERLCLpPE2ScO"
   },
   "source": [
    "A user may also easily obtain the Jacobian of the vector using the `get_jacobian(vector, variables)` method. `variables` is a list of the variable names in the order in which the partial derivatives should be organized in the resulting Jacobian matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "f0q5rsvl4haG",
    "outputId": "d52d0e4c-cd3c-4795-a325-b3f95670a1c2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 1.  0.  0.]\n",
      " [ 0. -1.  0.]\n",
      " [ 0.  0.  0.]]\n"
     ]
    }
   ],
   "source": [
    "x = ad.create_vector('x', [1, 2, 3])\n",
    "y = ad.create_vector('y', [5, 8, -7])\n",
    "z = x - y\n",
    "\n",
    "jacobian = ad.get_jacobian(z, ['x1', 'y2', 't'])\n",
    "print(jacobian); #notice that derivative with respect to 't' is all zeroes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3AL-waq24xpx"
   },
   "source": [
    "The vector of *Scalar* objects is important to the implementation of our optimization methods below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ecA1TfnFfSWw"
   },
   "source": [
    "## Optimization Functions\n",
    "\n",
    "As shown from the use case in the **Usage** section, this package is intended for use without the need for any knowledge regarding auto-differentiation. For each of the optimization methods, the user simply must pass in a function and a list of floats specifying the initial guess position. The user-defined function should only take one argument: a list/array of the same length as the initial guess list.\n",
    "\n",
    "All of the optimization methods in the package rely on the functions `create_vector`, `get_jacobian`, and `get_value` in order to get the necessary values for calculating the step in each iteration. All the optimization methods have been faithfully implemented according to their algorithms. \n",
    "\n",
    "We have implemented gradient descent and the quasi-Newton methods *DFP*, *BFGS*, and *Broyden* to solve optimization problems. They are all iterative algorithms that rely on an update of the form $x_{k+1} = x_k + \\Delta x_k$.\n",
    "* For gradient descent, in each iteration the step is defined by $\\Delta x_k = - stepsize * \\nabla f(x_k)$. The user can change the step size, max number of iterations, and tolerance. \n",
    "* For the quasi-Newton methods, \n",
    "$\\Delta x_k = -\\alpha_k H_k \\nabla f(x_k)$.\n",
    "The only difference between *DFP*, *BFGS*, and *Broyden* is the update formula for the estimate $H_k$ of $H_f(x_k)^{-1}$, the inverse of the Hessian. In order to solve for $\\alpha_k$ in each iteration, we also implemented a basic backtracking line search that follows the algorithm (4). The user can also specify the max number of iterations and tolerance for these quasi-Newton methods. \n",
    "\n",
    "\n",
    "We have also implemented Newton's method for root-finding using four different methods for determining the step size at each iteration. The step size $\\Delta x_k$ at iteration $k$ can be determined by solving the equation $J_f(x_k) \\Delta x_k = -f(x_k)$ which is of the form $Ax=b$ with $A$ being a given matrix and $b$ a given vector. $\\Delta x_k$ can be calculated using:\n",
    "* the inverse of $A$ (specified as `'inverse'`)\n",
    "* `np.linalg.solve` which first factorizes $A$ using LU decomposition, then solves for $x$ using forward and backward substitution (specified as `'exact'`)\n",
    "* `scipy`'s generalized minimal residual method `gmres`, passing the $A$ matrix in directly (specified as `'gmres'`)\n",
    "* `gmres` from `scipy` and passing in the action of $A$, i.e. a function $f$ such that $f(x) = Ax$ (specified as `'gmres_action'`)\n",
    "\n",
    "## Additional implementation\n",
    "\n",
    "We did not implement the operators **__lt__**, **__gt__**, **__le__**, and **__ge__** for comparing two *Scalar* objects. These operators would most likely be implemented in the next release of this package."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X31uNbPq5pNe"
   },
   "source": [
    "# Future plans\n",
    "\n",
    "In the future , we would also like to calculate the Hessian while doing auto-differentiation. Since manually calculating the Hessian during optimization methods can be computationally expensive, using auto-differentiation to calculate the Hessian would be more efficient and feasible. We can then implement Newton's method for minimization, since that requires the Hessian.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-qSq1PRI5pNf"
   },
   "source": [
    "# Citations\n",
    "1. Sondak, David. “Automatic Differentiation: The Basics.” CS207-Lecture9. Cambridge, MA. 2 October 2018.\n",
    "\n",
    "2. Hoffman, Philipp H.W. “A Hitchhiker’s Guide to Automatic Differentiation.” *Numerical Algorithms*, 72, 24 October 2015, 775-811, *Springer Link*, DOI 10.1007/s11075-015-0067-6. \n",
    "\n",
    "3. \"Quasi-Newton method.\" https://en.wikipedia.org/wiki/Quasi-Newton_method. Wikipedia. 10 December 2018.\n",
    "\n",
    "4. \"Backtracking line search.\" https://en.wikipedia.org/wiki/Backtracking_line_search. Wikipedia. 10 December 2018."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Final_milestone_cs207.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
