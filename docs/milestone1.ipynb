{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "milestone1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "metadata": {
        "id": "g5t4k5oB1pr5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Introduction\n",
        "\n",
        "Since Newton invented calculus, differentiation has been essential to the advancement of humanity. Calculating the derivative of a function is crucial to finding the extrema for a function and determining zeros for a function, two operations that are central to optimization. Often, we can find the analytical solution to the derivative of a function, however this has become increasingly complex as our functions/equations have grown in size and complexity. Numerically solving differential equations forms a cornerstone of modern science and engineering and is intimately linked with predictive science.\n"
      ]
    },
    {
      "metadata": {
        "id": "v4txoXsH2Dsx",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Background\n",
        "\n",
        "The mathematical background and concepts that may fit in are:\n",
        "- The chain rule\n",
        "$$\\dfrac{\\partial h}{\\partial t} = \\dfrac{\\partial h}{\\partial u}\\dfrac{\\partial u}{\\partial t}.$$\n",
        "- The gradient\n",
        "\\begin{align}\n",
        "  \\nabla_{x}h = \\sum_{i=1}^{n}{\\frac{\\partial h}{\\partial y_{i}}\\nabla y_{i}\\left(x\\right)}\n",
        "\\end{align}\n",
        "\n",
        "- The graph structure of calculations and forward accumulation\n",
        "\n",
        "- Differential calculus\n",
        "\n",
        "- Elementary functions (log, cos, sin, exp, ...)\n",
        "\n",
        "|       Function $f(x)$                |       Derivative $f^{\\prime}(x)$                |\n",
        "| :-------------------:  | :------------------------------------------------------------------------------:  |\n",
        "| ${c}$           | $0$         |\n",
        "| ${x}$           | $1$         |\n",
        "| ${x^{n}}$           | ${nx^{n-1}}$         |\n",
        "| $\\frac{1}{x}$     | $\\frac{-1}{x^{2}}$     |\n",
        "| $ln{x}$     | $\\frac{1}{x}$     |\n",
        "| $\\sin(x)$           |   $\\cos(x)$         |\n",
        "| $\\cos(x)$           |   $-\\sin(x)$         |\n",
        "| $\\tan(x)$           |   $\\dfrac{1}{\\cos^2(x)}$         |\n",
        "| $\\exp(x)$           |   $\\exp(x)$         |\n",
        "| ${a^{x}}$           |   ${a^{x}\\ln{a}}$         |\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "uF4IjE5E2M77",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "#How to Use *adiff*\n",
        "\n",
        "How do you envision that a user will interact with your package?  What should they import?  How can\n",
        "they instantiate AD objects?\n",
        "\n",
        "Ideally, a user should not be overwhelmed by our package. They would only need to import our package and instantiate the basic variables or vectors (i.e. $x_1, x_2, x_3,  \\vec{x}$, etc.). Using the AD objects, the users will be able to easily call operations such as addition, subtraction, sine, exponential, etc together with both other AD objects and also numbers, forming the expressions that they want. The AD objects will feel perfectly integrated, and the users can get the value/derivatives of the expressions with basic get calls. \n",
        "\n",
        "For example, in the case of scalar functions/values, the user will type something like this:\n",
        "\n",
        "```python\n",
        "\n",
        "import autodiff as ad\n",
        "\n",
        "x = ad.Scalar('x', 2)\n",
        "x.get_value()\n",
        ">>>2\n",
        "x.get_deriv()\n",
        ">>>{'x': 1}\n",
        "y = ad.Scalar('y', 5)\n",
        "y.get_value()\n",
        ">>>5\n",
        "y.get_deriv()\n",
        ">>>{'y': 1}\n",
        "z  = x + y\n",
        "z.get_value() \n",
        ">>> 7\n",
        "z.get_deriv()\n",
        ">>> {'x': 1, 'y': 1}\n",
        "z.get_deriv('x')\n",
        ">>> 1\n",
        "z2 = x*y  \n",
        "z2.get_value()\n",
        ">>>10\n",
        "z2.get_deriv()\n",
        ">>> {'x': 5 , 'y': 2 }\n",
        "z3 = y ** 2 \n",
        "z3.get_value() \n",
        ">>> 25\n",
        "z3.get_deriv()\n",
        ">>>{'y': 10}\n",
        "z4 = ad.sin(ad.Scalar('x1',  0))\n",
        "z4.get_value()\n",
        ">>> 0\n",
        "z4.get_deriv()\n",
        ">>> 1\n",
        "```\n",
        "\n",
        "This idea can be extended to the cases of vectors.\n",
        "```python\n",
        "#do not need to follow this naming convention\n",
        "x1 = ad.Scalar('x1', 10) \n",
        "x2 = ad.Scalar('x2', 4)\n",
        "x = ad.Vector([x1, x2, x1 + x2, x1 * x2])\n",
        "x.get_values()\n",
        ">>> np.array([10, 4, 14, 40])\n",
        "x.get_values(row = 0)\n",
        ">>> 10\n",
        "#There can be a function to return it as matrix\n",
        "x.get_derivs()\n",
        ">>> [{'x1': 1, 'x2': 0}, {'x1': 0, 'x2': 1}, {'x1': 1, 'x2': 1}, {'x1': 4, 'x2': 10}]\n",
        "x.get_derivs('x1')\n",
        ">>> np.array([1, 0, 1, 4])\n",
        "```\n"
      ]
    },
    {
      "metadata": {
        "id": "qxhksP6w2Tv8",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Software Organization\n",
        "       \n",
        " * The directory structure would look like that:      \n",
        " ```\n",
        " autodiff\\\n",
        "          autodiff\\\n",
        "                    __init__.py\n",
        "                    variables.py (scalar and vector)\n",
        "                    functions.py\n",
        "                    test\\\n",
        "                         __init__.py\n",
        "                         test_variables.py\n",
        "                         test_functions.py\n",
        "          README.md\n",
        "          setup.py\n",
        "          LICENSE\n",
        "               \n",
        " ```            \n",
        "\n",
        "The plans on organizing our software package are: \n",
        "\n",
        "* We are planning to use numpy to conduct most of our calculations, vector operations and definition of elementary functions.\n",
        "* The test suite will live in the test directory shown above. We are using both `TravisCI` and `Coveralls'\n",
        "* We want to release our package in `PyPI`"
      ]
    },
    {
      "metadata": {
        "id": "Q5mj2XEc2Wv9",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Implementation"
      ]
    },
    {
      "metadata": {
        "id": "VKExRAqeKx09",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "Our core data structure for implementing the forward mode of automatic differentiation will be based on the *Scalar* and *Vector* classes. The *Scalar* class will hold two attributes: 1) the value of the variable at the current step and 2) a dictionary containing the derivative or partial derivatives (keys will be the names of the variables (i.e *x* and *y*) and the values will be the derivative value with respect to each variable).  The dunder methods __add__, __sub__, __mul__,  __truediv__, __pow__, __iadd__, __isub__, __imul__, __idiv__, __ipow__ (and the right equivalents) will all be overwritten so that they return a new *Scalar* object with an updated value and derivative. \n",
        "\n",
        "Another class called *Vector* will take in a list or array of *Scalar* objects. A *Vector* only has one attribute: a numpy array of *Scalar* objects, since each *Scalar* object will track its current value and derivative. The dunder methods __add__, __sub__, __mul__,  __truediv__, __pow__, __iadd__, __isub__, __imul__, __idiv__, __ipow__ (and the right equivalents) will all be overwritten so that they return a new array of *Scalar* objects with updates values and derivatives.\n",
        "\n",
        "We will implement functions *sin, cos, tan, arcsin, arccos, arctan, exp* (e^x), *power* (a^x), and *abs* (absolute value). The functions will not be implemented in a specific class, similar to the implementation of *numpy* . These functions will be overloaded so that if a *Scalar* object is passed in, then a new *Scalar* object with an updated value and derivative is returned, depending on the function being called. If a *Vector* object is passed in, then a new *Vector* object with updated values and derivatives is returned. If one of the functions is not differentiable at a given value, then we will throw an error for the user and explain that the function is not differentiable at this specific value.\n",
        "\n",
        "The implementation of our classes and functions will rely on *numpy*."
      ]
    }
  ]
}